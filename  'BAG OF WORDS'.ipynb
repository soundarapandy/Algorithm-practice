{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "     'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['some of the data are important',\n",
    "       'data is important in sometimes',\n",
    "       'is ths data is important',\n",
    "       'importance of data in calculated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['are', 'calculated', 'data', 'importance', 'important', 'in', 'is', 'of', 'some', 'sometimes', 'the', 'ths']\n",
      "[[1 0 1 0 1 0 0 1 1 0 1 0]\n",
      " [0 0 1 0 1 1 1 0 0 1 0 0]\n",
      " [0 0 1 0 1 0 2 0 0 0 0 1]\n",
      " [0 1 1 1 0 1 0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(data)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "soil = ['which is one?,',\n",
    "      'one of these which/.',\n",
    "      'which of these one?',\n",
    "      'which one is this?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'of', 'one', 'these', 'this', 'which']\n",
      "[[1 0 1 0 0 1]\n",
      " [0 1 1 1 0 1]\n",
      " [0 1 1 1 0 1]\n",
      " [1 0 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "vector = CountVectorizer()\n",
    "X = vector.fit_transform(soil)\n",
    "print(vector.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "import pickle \n",
    "#import mglearn\n",
    "import time\n",
    "\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer # doesn't split at apostrophes\n",
    "import nltk\n",
    "from nltk import Text\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  [\"He is ::having a great Time, at the park time?\",\n",
    "       \"She, unlike most women, is a big player on the park's grass.\",\n",
    "       \"she can't be going\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every_feature:\n",
      " ['big', 'going', 'grass', 'great', 'having', 'park', 'player', 'time', 'unlike', 'women']\n",
      "Total_array:\n",
      " [[0 0 0 1 1 1 0 2 0 0]\n",
      " [1 0 1 0 0 1 1 0 1 1]\n",
      " [0 1 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "count = CountVectorizer(stop_words='english', analyzer='word',ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None)\n",
    "X = count.fit_transform(data)\n",
    "print('Every_feature:\\n',count.get_feature_names())\n",
    "print('Total_array:\\n',X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every 3rd feature:\n",
      " ['big', 'great', 'player', 'women']\n"
     ]
    }
   ],
   "source": [
    "#print every three features\n",
    "print('Every 3rd feature:\\n',count.get_feature_names()[::3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary_size:\n",
      " 10\n",
      "vocabulary content:\n",
      " {'having': 4, 'great': 3, 'time': 7, 'park': 5, 'unlike': 8, 'women': 9, 'big': 0, 'player': 6, 'grass': 2, 'going': 1}\n"
     ]
    }
   ],
   "source": [
    "#print vocabulary features \n",
    "print('vocabulary_size:\\n',len(count.vocabulary_))\n",
    "print('vocabulary content:\\n',count.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every_feature:\n",
      " ['big', 'big player', 'going', 'grass', 'great', 'great time', 'having', 'having great', 'park', 'park grass', 'park time', 'player', 'player park', 'time', 'time park', 'unlike', 'unlike women', 'women', 'women big']\n",
      "Total_array:\n",
      " [[0 0 0 0 1 1 1 1 1 0 1 0 0 2 1 0 0 0 0]\n",
      " [1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 1 1 1 1]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#bi_gram\n",
    "count = CountVectorizer(stop_words='english', analyzer='word',ngram_range=(1, 2), max_df=1.0, min_df=1, max_features=None)\n",
    "X = count.fit_transform(data)\n",
    "print('Every_feature:\\n',count.get_feature_names())\n",
    "print('Total_array:\\n',X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every_feature:\n",
      " ['big', 'big player', 'big player park', 'going', 'grass', 'great', 'great time', 'great time park', 'having', 'having great', 'having great time', 'park', 'park grass', 'park time', 'player', 'player park', 'player park grass', 'time', 'time park', 'time park time', 'unlike', 'unlike women', 'unlike women big', 'women', 'women big', 'women big player']\n",
      "Total_array:\n",
      " [[0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 2 1 1 0 0 0 0 0 0]\n",
      " [1 1 1 0 1 0 0 0 0 0 0 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#tri_gram\n",
    "count = CountVectorizer(stop_words='english', analyzer='word',ngram_range=(1, 3), max_df=1.0, min_df=1, max_features=None)\n",
    "X = count.fit_transform(data)\n",
    "print('Every_feature:\\n',count.get_feature_names())\n",
    "print('Total_array:\\n',X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every_feature:\n",
      " ['park']\n",
      "Total_array:\n",
      " [[1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "#min_df\n",
    "count = CountVectorizer(stop_words='english',analyzer = 'word',ngram_range=(1, 1),max_df=1.0,min_df=0.6,max_features=None)\n",
    "X = count.fit_transform(data)\n",
    "print('Every_feature:\\n',count.get_feature_names())\n",
    "print('Total_array:\\n',X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every_feature:\n",
      " ['big', 'going', 'grass', 'great', 'having', 'player', 'time', 'unlike', 'women']\n",
      "Total_array:\n",
      " [[0 0 0 1 1 0 2 0 0]\n",
      " [1 0 1 0 0 1 0 1 1]\n",
      " [0 1 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#max_df\n",
    "count = CountVectorizer(stop_words='english',analyzer = 'word',ngram_range=(1, 1),max_df=0.50,min_df=1,max_features=None)\n",
    "X = count.fit_transform(data)\n",
    "print('Every_feature:\\n',count.get_feature_names())\n",
    "print('Total_array:\\n',X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every_feature:\n",
      " ['big', 'going', 'grass', 'time']\n",
      "Total_array:\n",
      " [[0 0 0 2]\n",
      " [1 0 1 0]\n",
      " [0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#max_features\n",
    "count = CountVectorizer(stop_words='english',analyzer = 'word',ngram_range=(1, 1),max_df=0.50,min_df=1,max_features=4)\n",
    "X = count.fit_transform(data)\n",
    "print('Every_feature:\\n',count.get_feature_names())\n",
    "print('Total_array:\\n',X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfidfVectorizer -- Brief Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt1 = ['His smile was not perfect', 'His smile was not not not not perfect', 'she not sang']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text:  ['His smile was not perfect', 'His smile was not not not not perfect', 'she not sang']\n"
     ]
    }
   ],
   "source": [
    "tf = TfidfVectorizer(smooth_idf=False, sublinear_tf=False, norm=None, analyzer='word')\n",
    "X = tf.fit_transform(txt1)\n",
    "print (\"The text: \", txt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: {'his': 0, 'smile': 5, 'was': 6, 'not': 1, 'perfect': 2, 'she': 4, 'sang': 3}\n"
     ]
    }
   ],
   "source": [
    "#vocabulary\n",
    "print('vocabulary:',tf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idf\n",
    "idf = tf.idf_\n",
    "#print(dict(zip(txt_fitted.get_feature_names(), idf)))\n",
    "\n",
    "#We see that the tokens 'sang','she' have the most idf weight because they are the only tokens that appear in one document only.\n",
    "\n",
    "#The token 'not' appears 6 times but it is also in all documents, so its idf is the lowest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['his', 'not', 'perfect', 'sang', 'she', 'smile', 'was'],\n",
       "      dtype='<U7')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get feature names\n",
    "feature_names = np.array(tf.get_feature_names())\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 5, 6, 3, 4], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_by_idf = np.argsort(tf.idf_)\n",
    "sorted_by_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lower idf:\n",
      " ['not' 'his' 'perfect']\n",
      "features with higher idf:\n",
      " ['was' 'sang' 'she']\n"
     ]
    }
   ],
   "source": [
    "#sort by idf\n",
    "print('Features with lower idf:\\n',feature_names[sorted_by_idf[:3]])\n",
    "print('features with higher idf:\\n',feature_names[sorted_by_idf[-3:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token 'not' has  the largest weight in document #2 because it appears 3 times there. But in document #1 its weight is 0 because it does not appear there.\n"
     ]
    }
   ],
   "source": [
    "print(\"The token 'not' has  the largest weight in document #2 because it appears 3 times there. But in document #1\\\n",
    " its weight is 0 because it does not appear there.\")\n",
    "#txt_transformed.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lowest tfidf:\n",
      "['his' 'perfect' 'smile']\n",
      "\n",
      "Features with highest tfidf: \n",
      "['sang' 'she' 'not']\n"
     ]
    }
   ],
   "source": [
    "new1 = tf.transform(txt1)\n",
    "\n",
    "# find maximum value for each of the features over all of dataset:\n",
    "max_val = new1.max(axis=0).toarray().ravel()\n",
    "\n",
    "#sort weights from smallest to biggest and extract their indices \n",
    "sort_by_tfidf = max_val.argsort()\n",
    "\n",
    "print(\"Features with lowest tfidf:\\n{}\".format(\n",
    "      feature_names[sort_by_tfidf[:3]]))\n",
    "\n",
    "print(\"\\nFeatures with highest tfidf: \\n{}\".format(\n",
    "      feature_names[sort_by_tfidf[-3:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB,  MultinomialNB, BernoulliNB\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1,1,1],\n",
    "            [0,0,0],\n",
    "            [2,2,2]])\n",
    "y = np.array(['GOOD','BAD','WORST'])\n",
    "predict_this = [0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB: ['BAD']\n"
     ]
    }
   ],
   "source": [
    "#GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(x,y)\n",
    "predicted = model.predict([predict_this])\n",
    "print('GaussianNB:',predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB: ['GOOD']\n"
     ]
    }
   ],
   "source": [
    "#MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model.fit(x,y)\n",
    "predicted = model.predict([predict_this])\n",
    "print('MultinomialNB:',predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseNB: ['BAD']\n"
     ]
    }
   ],
   "source": [
    "#BaseNB\n",
    "model = BernoulliNB()\n",
    "model.fit(x,y)\n",
    "predicted = model.predict([predict_this])\n",
    "print('BaseNB:',predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: [1]\n"
     ]
    }
   ],
   "source": [
    "#Numerical_prediction\n",
    "x = np.array([[5,0,1,3],[4,3,1,2]])\n",
    "y = np.array([1,0])\n",
    "model = GaussianNB()\n",
    "model.fit(x,y)\n",
    "predicted = model.predict([[3,0,3,1]])\n",
    "print('predicted:',predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: [1]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[5,0,1,3],[4,3,1,2]])\n",
    "y = np.array([1,0])\n",
    "model = MultinomialNB()\n",
    "model.fit(x,y)\n",
    "predicted = model.predict([[3,0,3,1]])\n",
    "print('predicted:',predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: [1]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[5,0,1,3],[4,3,1,2]])\n",
    "y = np.array([1,0])\n",
    "model = BernoulliNB()\n",
    "model.fit(x,y)\n",
    "predicted = model.predict([[3,0,3,1]])\n",
    "print('predicted:',predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: [1]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[2,2,2,2],[1,3,4,5]])\n",
    "y = np.array([1,0])\n",
    "model = GaussianNB()\n",
    "model.fit(x,y)\n",
    "predicted = model.predict([[2,4,2,2]])\n",
    "print('predicted:',predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=[\"the house had a tiny little mouse\",\n",
    "      \"the cat saw the mouse\",\n",
    "      \"the mouse ran away from the house\",\n",
    "      \"the cat finally ate the mouse\",\n",
    "      \"the end of the mouse story\"\n",
    "     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1]\n",
      " [0 0 1 0 0 0 0 0 0 1 0 0 1 0 2 0]\n",
      " [0 1 0 0 0 1 0 1 0 1 0 1 0 0 2 0]\n",
      " [1 0 1 0 1 0 0 0 0 1 0 0 0 0 2 0]\n",
      " [0 0 0 1 0 0 0 0 0 1 1 0 0 1 2 0]]\n",
      "['ate', 'away', 'cat', 'end', 'finally', 'from', 'had', 'house', 'little', 'mouse', 'of', 'ran', 'saw', 'story', 'the', 'tiny']\n"
     ]
    }
   ],
   "source": [
    "#instantiate CountVectorizer()\n",
    "cv=CountVectorizer()\n",
    " \n",
    "# this steps generates word counts for the words in your docs\n",
    "word_count_vector=cv.fit_transform(docs)\n",
    "word_count_vector.shape\n",
    "print(word_count_vector.toarray())\n",
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idf_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ate</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finally</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>had</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house</th>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>little</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mouse</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ran</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saw</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>story</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiny</th>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         idf_weights\n",
       "ate         2.098612\n",
       "away        2.098612\n",
       "cat         1.693147\n",
       "end         2.098612\n",
       "finally     2.098612\n",
       "from        2.098612\n",
       "had         2.098612\n",
       "house       1.693147\n",
       "little      2.098612\n",
       "mouse       1.000000\n",
       "of          2.098612\n",
       "ran         2.098612\n",
       "saw         2.098612\n",
       "story       2.098612\n",
       "the         1.000000\n",
       "tiny        2.098612"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf = pd.DataFrame(transformer.idf_,index=cv.get_feature_names(),columns=[\"idf_weights\"])\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 15)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 14)\t2\n",
      "  (2, 1)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 9)\t1\n",
      "  (2, 11)\t1\n",
      "  (2, 14)\t2\n",
      "  (3, 0)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 4)\t1\n",
      "  (3, 9)\t1\n",
      "  (3, 14)\t2\n",
      "  (4, 3)\t1\n",
      "  (4, 9)\t1\n",
      "  (4, 10)\t1\n",
      "  (4, 13)\t1\n",
      "  (4, 14)\t2\n"
     ]
    }
   ],
   "source": [
    "count_vector = cv.transform(docs)\n",
    "tf_idf_vector =transformer.transform(count_vector)\n",
    "print(count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>had</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>little</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiny</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house</th>\n",
       "      <td>0.398203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mouse</th>\n",
       "      <td>0.235185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.235185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ate</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finally</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ran</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saw</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>story</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tfidf\n",
       "had      0.493562\n",
       "little   0.493562\n",
       "tiny     0.493562\n",
       "house    0.398203\n",
       "mouse    0.235185\n",
       "the      0.235185\n",
       "ate      0.000000\n",
       "away     0.000000\n",
       "cat      0.000000\n",
       "end      0.000000\n",
       "finally  0.000000\n",
       "from     0.000000\n",
       "of       0.000000\n",
       "ran      0.000000\n",
       "saw      0.000000\n",
       "story    0.000000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = cv.get_feature_names()\n",
    " \n",
    "#get tfidf vector for first document\n",
    "first_document_vector=tf_idf_vector[0]\n",
    " \n",
    "#print the scores\n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tdidf_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>had</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>little</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiny</th>\n",
       "      <td>0.493562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house</th>\n",
       "      <td>0.398203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mouse</th>\n",
       "      <td>0.235185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.235185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ate</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finally</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ran</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saw</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>story</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tdidf_values\n",
       "had          0.493562\n",
       "little       0.493562\n",
       "tiny         0.493562\n",
       "house        0.398203\n",
       "mouse        0.235185\n",
       "the          0.235185\n",
       "ate          0.000000\n",
       "away         0.000000\n",
       "cat          0.000000\n",
       "end          0.000000\n",
       "finally      0.000000\n",
       "from         0.000000\n",
       "of           0.000000\n",
       "ran          0.000000\n",
       "saw          0.000000\n",
       "story        0.000000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tdidf = TfidfVectorizer(use_idf=True)\n",
    "tf_idf = tdidf.fit_transform(docs)\n",
    "\n",
    "feature_first_vector = tf_idf[0]\n",
    "df = pd.DataFrame(feature_first_vector.T.todense(),index =tdidf.get_feature_names(),columns=['tdidf_values'])\n",
    "df.sort_values(by=['tdidf_values'],ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit: {'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "feature_names: ['brown', 'dog', 'fox', 'jumped', 'lazy', 'over', 'quick', 'the']\n",
      "shape: (1, 8)\n",
      "array_values: [[1 1 1 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text = ['The quick brown fox jumped over the lazy dog']\n",
    "count = CountVectorizer()\n",
    "vector = count.fit(text)\n",
    "print('fit:',count.vocabulary_)\n",
    "print('feature_names:',vector.get_feature_names())\n",
    "counts = vector.transform(text)\n",
    "print('shape:',counts.shape)\n",
    "print('array_values:',counts.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t0.6030226891555273\n",
      "  (0, 6)\t0.30151134457776363\n",
      "  (0, 0)\t0.30151134457776363\n",
      "  (0, 2)\t0.30151134457776363\n",
      "  (0, 3)\t0.30151134457776363\n",
      "  (0, 5)\t0.30151134457776363\n",
      "  (0, 4)\t0.30151134457776363\n",
      "  (0, 1)\t0.30151134457776363\n",
      "['brown', 'dog', 'fox', 'jumped', 'lazy', 'over', 'quick', 'the']\n",
      "[[0.30151134 0.30151134 0.30151134 0.30151134 0.30151134 0.30151134\n",
      "  0.30151134 0.60302269]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "count = TfidfVectorizer()\n",
    "vector = count.fit_transform(text)\n",
    "print(vector)\n",
    "print(count.get_feature_names())\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
