{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB,  MultinomialNB, BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "     'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['some of the data are important',\n",
    "       'data is important in sometimes',\n",
    "       'is ths data is important',\n",
    "       'importance of data in calculated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['are', 'calculated', 'data', 'importance', 'important', 'in', 'is', 'of', 'some', 'sometimes', 'the', 'ths']\n",
      "[[1 0 1 0 1 0 0 1 1 0 1 0]\n",
      " [0 0 1 0 1 1 1 0 0 1 0 0]\n",
      " [0 0 1 0 1 0 2 0 0 0 0 1]\n",
      " [0 1 1 1 0 1 0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(data)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "soil = ['which is one?,',\n",
    "      'one of these which/.',\n",
    "      'which of these one?',\n",
    "      'which one is this?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'of', 'one', 'these', 'this', 'which']\n",
      "[[1 0 1 0 0 1]\n",
      " [0 1 1 1 0 1]\n",
      " [0 1 1 1 0 1]\n",
      " [1 0 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "vector = CountVectorizer()\n",
    "X = vector.fit_transform(soil)\n",
    "print(vector.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =  [\"He is ::having a great Time, at the park time?\",\n",
    "       \"She, unlike most women, is a big player on the park's grass.\",\n",
    "       \"she can't be going\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every_feature:\n",
      " ['big', 'going', 'grass', 'great', 'having', 'park', 'player', 'time', 'unlike', 'women']\n",
      "Total_array:\n",
      " [[0 0 0 1 1 1 0 2 0 0]\n",
      " [1 0 1 0 0 1 1 0 1 1]\n",
      " [0 1 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "count = CountVectorizer(stop_words='english', analyzer='word',ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None)\n",
    "X = count.fit_transform(data)\n",
    "print('Every_feature:\\n',count.get_feature_names())\n",
    "print('Total_array:\\n',X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every 3rd feature:\n",
      " ['big', 'great', 'player', 'women']\n"
     ]
    }
   ],
   "source": [
    "#print every three features\n",
    "print('Every 3rd feature:\\n',count.get_feature_names()[::3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary_size:\n",
      " 10\n",
      "vocabulary content:\n",
      " {'having': 4, 'great': 3, 'time': 7, 'park': 5, 'unlike': 8, 'women': 9, 'big': 0, 'player': 6, 'grass': 2, 'going': 1}\n"
     ]
    }
   ],
   "source": [
    "#print vocabulary features \n",
    "print('vocabulary_size:\\n',len(count.vocabulary_))\n",
    "print('vocabulary content:\\n',count.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every_feature:\n",
      " ['big', 'big player', 'going', 'grass', 'great', 'great time', 'having', 'having great', 'park', 'park grass', 'park time', 'player', 'player park', 'time', 'time park', 'unlike', 'unlike women', 'women', 'women big']\n",
      "Total_array:\n",
      " [[0 0 0 0 1 1 1 1 1 0 1 0 0 2 1 0 0 0 0]\n",
      " [1 1 0 1 0 0 0 0 1 1 0 1 1 0 0 1 1 1 1]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#bi_gram\n",
    "count = CountVectorizer(stop_words='english', analyzer='word',ngram_range=(1, 2), max_df=1.0, min_df=1, max_features=None)\n",
    "X = count.fit_transform(data)\n",
    "print('Every_feature:\\n',count.get_feature_names())\n",
    "print('Total_array:\\n',X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every_feature:\n",
      " ['big', 'big player', 'big player park', 'going', 'grass', 'great', 'great time', 'great time park', 'having', 'having great', 'having great time', 'park', 'park grass', 'park time', 'player', 'player park', 'player park grass', 'time', 'time park', 'time park time', 'unlike', 'unlike women', 'unlike women big', 'women', 'women big', 'women big player']\n",
      "Total_array:\n",
      " [[0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 2 1 1 0 0 0 0 0 0]\n",
      " [1 1 1 0 1 0 0 0 0 0 0 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#tri_gram\n",
    "count = CountVectorizer(stop_words='english', analyzer='word',ngram_range=(1, 3), max_df=1.0, min_df=1, max_features=None)\n",
    "X = count.fit_transform(data)\n",
    "print('Every_feature:\\n',count.get_feature_names())\n",
    "print('Total_array:\\n',X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every_feature:\n",
      " ['park']\n",
      "Total_array:\n",
      " [[1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "#min_df\n",
    "count = CountVectorizer(stop_words='english',analyzer = 'word',ngram_range=(1, 1),max_df=1.0,min_df=0.6,max_features=None)\n",
    "X = count.fit_transform(data)\n",
    "print('Every_feature:\\n',count.get_feature_names())\n",
    "print('Total_array:\\n',X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every_feature:\n",
      " ['big', 'going', 'grass', 'great', 'having', 'player', 'time', 'unlike', 'women']\n",
      "Total_array:\n",
      " [[0 0 0 1 1 0 2 0 0]\n",
      " [1 0 1 0 0 1 0 1 1]\n",
      " [0 1 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#max_df\n",
    "count = CountVectorizer(stop_words='english',analyzer = 'word',ngram_range=(1, 1),max_df=0.50,min_df=1,max_features=None)\n",
    "X = count.fit_transform(data)\n",
    "print('Every_feature:\\n',count.get_feature_names())\n",
    "print('Total_array:\\n',X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every_feature:\n",
      " ['big', 'going', 'grass', 'time']\n",
      "Total_array:\n",
      " [[0 0 0 2]\n",
      " [1 0 1 0]\n",
      " [0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#max_features\n",
    "count = CountVectorizer(stop_words='english',analyzer = 'word',ngram_range=(1, 1),max_df=0.50,min_df=1,max_features=4)\n",
    "X = count.fit_transform(data)\n",
    "print('Every_feature:\\n',count.get_feature_names())\n",
    "print('Total_array:\\n',X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfidfVectorizer -- Brief Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt1 = ['His smile was not perfect', 'His smile was not not not not perfect', 'she not sang']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text:  ['His smile was not perfect', 'His smile was not not not not perfect', 'she not sang']\n"
     ]
    }
   ],
   "source": [
    "tf = TfidfVectorizer(smooth_idf=False, sublinear_tf=False, norm=None, analyzer='word')\n",
    "X = tf.fit_transform(txt1)\n",
    "print (\"The text: \", txt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary: {'his': 0, 'smile': 5, 'was': 6, 'not': 1, 'perfect': 2, 'she': 4, 'sang': 3}\n"
     ]
    }
   ],
   "source": [
    "#vocabulary\n",
    "print('vocabulary:',tf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idf\n",
    "idf = tf.idf_\n",
    "#print(dict(zip(txt_fitted.get_feature_names(), idf)))\n",
    "\n",
    "#We see that the tokens 'sang','she' have the most idf weight because they are the only tokens that appear in one document only.\n",
    "\n",
    "#The token 'not' appears 6 times but it is also in all documents, so its idf is the lowest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['his', 'not', 'perfect', 'sang', 'she', 'smile', 'was'],\n",
       "      dtype='<U7')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get feature names\n",
    "feature_names = np.array(tf.get_feature_names())\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 5, 6, 3, 4], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_by_idf = np.argsort(tf.idf_)\n",
    "sorted_by_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lower idf:\n",
      " ['not' 'his' 'perfect']\n",
      "features with higher idf:\n",
      " ['was' 'sang' 'she']\n"
     ]
    }
   ],
   "source": [
    "#sort by idf\n",
    "print('Features with lower idf:\\n',feature_names[sorted_by_idf[:3]])\n",
    "print('features with higher idf:\\n',feature_names[sorted_by_idf[-3:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token 'not' has  the largest weight in document #2 because it appears 3 times there. But in document #1 its weight is 0 because it does not appear there.\n"
     ]
    }
   ],
   "source": [
    "print(\"The token 'not' has  the largest weight in document #2 because it appears 3 times there. But in document #1\\\n",
    " its weight is 0 because it does not appear there.\")\n",
    "#txt_transformed.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with lowest tfidf:\n",
      "['his' 'perfect' 'smile']\n",
      "\n",
      "Features with highest tfidf: \n",
      "['sang' 'she' 'not']\n"
     ]
    }
   ],
   "source": [
    "new1 = tf.transform(txt1)\n",
    "\n",
    "# find maximum value for each of the features over all of dataset:\n",
    "max_val = new1.max(axis=0).toarray().ravel()\n",
    "\n",
    "#sort weights from smallest to biggest and extract their indices \n",
    "sort_by_tfidf = max_val.argsort()\n",
    "\n",
    "print(\"Features with lowest tfidf:\\n{}\".format(\n",
    "      feature_names[sort_by_tfidf[:3]]))\n",
    "\n",
    "print(\"\\nFeatures with highest tfidf: \\n{}\".format(\n",
    "      feature_names[sort_by_tfidf[-3:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1,1,1],\n",
    "            [0,0,0],\n",
    "            [2,2,2]])\n",
    "y = np.array(['GOOD','BAD','WORST'])\n",
    "predict_this = [0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB: ['BAD']\n"
     ]
    }
   ],
   "source": [
    "#GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(x,y)\n",
    "predicted = model.predict([predict_this])\n",
    "print('GaussianNB:',predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB: ['GOOD']\n"
     ]
    }
   ],
   "source": [
    "#MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model.fit(x,y)\n",
    "predicted = model.predict([predict_this])\n",
    "print('MultinomialNB:',predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseNB: ['BAD']\n"
     ]
    }
   ],
   "source": [
    "#BaseNB\n",
    "model = BernoulliNB()\n",
    "model.fit(x,y)\n",
    "predicted = model.predict([predict_this])\n",
    "print('BaseNB:',predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: [1]\n"
     ]
    }
   ],
   "source": [
    "#Numerical_prediction\n",
    "x = np.array([[5,0,1,3],[4,3,1,2]])\n",
    "y = np.array([1,0])\n",
    "model = GaussianNB()\n",
    "model.fit(x,y)\n",
    "predicted = model.predict([[3,0,3,1]])\n",
    "print('predicted:',predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: [1]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[5,0,1,3],[4,3,1,2]])\n",
    "y = np.array([1,0])\n",
    "model = MultinomialNB()\n",
    "model.fit(x,y)\n",
    "predicted = model.predict([[3,0,3,1]])\n",
    "print('predicted:',predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: [1]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[5,0,1,3],[4,3,1,2]])\n",
    "y = np.array([1,0])\n",
    "model = BernoulliNB()\n",
    "model.fit(x,y)\n",
    "predicted = model.predict([[3,0,3,1]])\n",
    "print('predicted:',predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: [1]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[2,2,2,2],[1,3,4,5]])\n",
    "y = np.array([1,0])\n",
    "model = GaussianNB()\n",
    "model.fit(x,y)\n",
    "predicted = model.predict([[2,4,2,2]])\n",
    "print('predicted:',predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=[\"the house had a tiny little mouse\",\n",
    "      \"the cat saw the mouse\",\n",
    "      \"the mouse ran away from the house\",\n",
    "      \"the cat finally ate the mouse\",\n",
    "      \"the end of the mouse story\"\n",
    "     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1]\n",
      " [0 0 1 0 0 0 0 0 0 1 0 0 1 0 2 0]\n",
      " [0 1 0 0 0 1 0 1 0 1 0 1 0 0 2 0]\n",
      " [1 0 1 0 1 0 0 0 0 1 0 0 0 0 2 0]\n",
      " [0 0 0 1 0 0 0 0 0 1 1 0 0 1 2 0]]\n",
      "['ate', 'away', 'cat', 'end', 'finally', 'from', 'had', 'house', 'little', 'mouse', 'of', 'ran', 'saw', 'story', 'the', 'tiny']\n"
     ]
    }
   ],
   "source": [
    "#instantiate CountVectorizer()\n",
    "cv=CountVectorizer()\n",
    " \n",
    "# this steps generates word counts for the words in your docs\n",
    "word_count_vector=cv.fit_transform(docs)\n",
    "word_count_vector.shape\n",
    "print(word_count_vector.toarray())\n",
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfTransformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-f4b2a5e378fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtransformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msmooth_idf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0muse_idf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_count_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TfidfTransformer' is not defined"
     ]
    }
   ],
   "source": [
    "transformer = TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = pd.DataFrame(transformer.idf_,index=cv.get_feature_names(),columns=[\"idf_weights\"])\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vector = cv.transform(docs)\n",
    "tf_idf_vector =transformer.transform(count_vector)\n",
    "print(count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = cv.get_feature_names()\n",
    " \n",
    "#get tfidf vector for first document\n",
    "first_document_vector=tf_idf_vector[0]\n",
    " \n",
    "#print the scores\n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdidf = TfidfVectorizer(use_idf=True)\n",
    "tf_idf = tdidf.fit_transform(docs)\n",
    "\n",
    "feature_first_vector = tf_idf[0]\n",
    "df = pd.DataFrame(feature_first_vector.T.todense(),index =tdidf.get_feature_names(),columns=['tdidf_values'])\n",
    "df.sort_values(by=['tdidf_values'],ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text = ['The quick brown fox jumped over the lazy dog']\n",
    "count = CountVectorizer()\n",
    "vector = count.fit(text)\n",
    "print('fit:',count.vocabulary_)\n",
    "print('feature_names:',vector.get_feature_names())\n",
    "counts = vector.transform(text)\n",
    "print('shape:',counts.shape)\n",
    "print('array_values:',counts.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = TfidfVectorizer()\n",
    "vector = count.fit_transform(text)\n",
    "print(vector)\n",
    "print(count.get_feature_names())\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
